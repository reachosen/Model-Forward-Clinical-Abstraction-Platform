import { S5Adapter } from '../adapters/S5Adapter';
import { DatasetLoader } from '../loaders/DatasetLoader';
import { SAFEEvaluator } from '../evaluators/SAFEEvaluator';
import { RefineryRunConfig, RefineryReport } from '../config/definitions';
import { DomainContext, StructuralSkeleton } from '../../../PlanningFactory/types';

export class RefineryRunner {
  constructor(
    private s5Adapter: S5Adapter,
    private datasetLoader: DatasetLoader,
    private evaluator: SAFEEvaluator
  ) {}

  async run(config: RefineryRunConfig): Promise<RefineryReport> {
    console.log(`üöß Refinery Run: ${config.run_id}`);
    console.log(`   Task: ${config.task_type}`);
    console.log(`   Candidate: ${config.candidate.version_label}`);

    const testCases = this.datasetLoader.loadGoldenSet(config.dataset_id);
    const casesToRun = config.sample_size ? testCases.slice(0, config.sample_size) : testCases;
    
    console.log(`   Loaded ${casesToRun.length} cases.`);

    const details: any[] = [];
    let passedCount = 0;
    let failedCount = 0;

    for (const tc of casesToRun) {
      // 1. Build Context from Test Case
      // This is a simplification; in a real app we'd map TestCase fields to DomainContext
      const context: DomainContext = {
        domain: 'Orthopedics', // Inferred from I25
        primary_archetype: 'Process_Auditor', // Default for now
        archetypes: ['Process_Auditor'],
        patient_payload: tc.patient_payload,
        semantic_context: {
          packet: {
            metric: {
              metric_name: config.metric_id,
              metric_type: 'outcome_oriented_process',
              clinical_focus: 'Evaluated by refinery',
              rationale: 'Testing',
              signal_groups: ['delay_drivers', 'outcome_risks', 'safety_signals', 'documentation_gaps'],
              risk_factors: [],
              review_questions: [],
              submetrics: [],
              priority_for_clinical_review: 'high'
            },
            signals: {},
            priorities: {} as any
          },
          ranking: undefined
        }
      };

      const skeleton: StructuralSkeleton = {
        plan_metadata: {
          plan_id: 'refinery_plan',
          concern: {
            concern_id: config.metric_id,
            concern_type: 'USNWR',
            domain: 'Orthopedics'
          }
        },
        clinical_config: { 
          signals: { 
            signal_groups: [] 
          } 
        }
      };

      // 2. Inject Payload
      // We cheat slightly: S5 expects patient_payload in "ortho_context" sometimes or "input".
      // Actually S5 prompts look at "patient_payload".
      // We need to inject it into the prompt context.
      // Our S5Adapter sets up the context.
      // But wait, getSignalEnrichmentVariables uses "ortho_context" and "archetype".
      // It DOES NOT take patient_payload as an argument.
      // The PROMPT TEXT has {{patient_payload}} placeholder?
      // No, look at signalEnrichment.ts: "Use ONLY the patient_payload as your factual source."
      // The System Prompt usually injects the payload.
      // S5_TaskExecution.callLLM sends: messages: [{ role: 'user', content: finalPrompt }]
      
      // ISSUE: The prompt text generated by loadPromptTemplate DOES NOT include the patient data.
      // In the real pipeline, where does patient data come from?
      // Ah, S5_TaskExecution -> callLLM -> messages.
      // But `buildMetricFramedPrompt` just builds the *System Instruction*.
      // Where is the *User Message* with the patient case?
      
      // Let's re-read S5_TaskExecution.ts carefully.
      // callLLM takes "prompt".
      // loadPromptTemplate returns a string.
      // The string returned by getSignalEnrichmentCoreBody is the *instruction*.
      // Where is the patient data appended?
      
      // It seems S5_TaskExecution logic assumes the prompt contains everything?
      // Or maybe `buildMetricFramedPrompt` implies it?
      // No, `buildMetricFramedPrompt` returns `ROLE: ... context ... body`.
      
      // Check `S5_TaskExecution.ts`:
      // const prompt = loadPromptTemplate(...)
      // output = await callLLM({ prompt, ... })
      
      // It seems the patient data is MISSING in the current S5 implementation analysis?
      // Or maybe it was expected to be part of the `context` passed to `loadPromptTemplate` and interpolated?
      
      // Let's look at `buildSystemPrompt` in `promptBuilder.ts`.
      // It says "You have access to ... patient_payload".
      
      // CRITICAL FINDING: The current S5 implementation *generates the system prompt* but *does not append the patient case*.
      // In the real app, there must be a step that appends the patient data.
      // OR, the `context` object passed to `loadPromptTemplate` contains `patient_payload` but it isn't used in the template?
      
      // I will assume for now that I need to append the patient payload to the prompt text.
      const fullPrompt = `${config.candidate.prompt_template}

PATIENT DATA:
${tc.patient_payload}`;

      try {
        const output = await this.s5Adapter.executeSingleTask(
          fullPrompt,
          config.task_type,
          context,
          skeleton
        );

        // 3. Evaluate
        const scorecard = this.evaluator.evaluate(
          output.output,
          config.task_type,
          config.metric_id,
          'Process_Auditor',
          config.run_id
        );

        if (scorecard.overall_label === 'Pass') passedCount++;
        else failedCount++;

        details.push({
          case_id: tc.test_id,
          output: output.output,
          scorecard
        });

      } catch (err) {
        console.error(`   ‚ùå Case ${tc.test_id} failed:`, err);
        failedCount++;
      }
    }

    // 4. Report
    return {
      run_id: config.run_id,
      overall_score: passedCount / (passedCount + failedCount),
      safe_scorecard: {} as any, // TODO: Aggregate logic
      passed_cases: passedCount,
      failed_cases: failedCount,
      details
    };
  }
}
